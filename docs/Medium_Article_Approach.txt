From Natural Language to Complex MongoDB Pipelines: A Graph-Driven AI Approach

By [Your Name]

In the modern data landscape, the ability to query information quickly is paramount. However, for non-technical users, writing complex MongoDB aggregation pipelines—especially those involving multiple joins, nested arrays, and transitive relationships—can be a daunting task.

We set out to build a system that bridges this gap: a Natural Language to MongoDB Query translator that doesn't just "guess" the query, but uses a robust, schema-aware architecture to compile valid, multi-stage pipelines.

### The Challenge: A Complex Real-World Schema

To truly test our approach, we applied it to a complex, multi-collection retail ecosystem. Our domain includes:

- **Orders & Items**: The central collection, featuring embedded arrays of products and complex status states.
- **Customers & Users**: Distinct entities representing the buyers and the staff members (like cashiers and drivers) who facilitate the orders.
- **Outlets & Products**: The physical locations and the inventory catalog.
- **Payments & Deliveries**: Secondary collections that link back to orders, creating multi-hop relationship chains.

### A Look at the Schema (JSON)

To illustrate the complexity, here is a representative schema for our central `orders` collection, showing the nested items and foreign key references:

```json
{
  "orders": {
    "_id": "ObjectId",
    "orderNo": "string",
    "customerId": "ObjectId",  // Refers to 'customers'
    "outletId": "ObjectId",   // Refers to 'outlets'
    "createdByUserId": "ObjectId", // Refers to 'users'
    "status": "string (PENDING, READY, etc.)",
    "createdAt": "date",
    "items": [
      {
        "productId": "ObjectId", // Refers to 'products'
        "qty": "number",
        "unitPrice": "number"
      }
    ],
    "delivery": {
      "assignedToUserId": "ObjectId", // Refers to 'users'
      "address": "string"
    }
  }
}
```

The real difficulty lies in the interconnectedness. A single query like *"List all deliveries handled by driver Ravi for orders containing Chocolate Cake"* requires joining four different collections: `deliveries`, `users`, `orders`, and `products` (via an embedded array). This is not just a search; it’s a graph traversal problem.

---

### The Architecture: A Three-Stage Pipeline

The core of our approach lies in decoupling "Intent" from "Implementation." We broke down the problem into three distinct stages:

1. Intent Generation (NLP to JSON)
2. Join Discovery (Neo4j Metadata Layer)
3. Query Compilation (MongoDB Aggregation)

### Stage 1: Capturing Intent with OpenAI Structured Outputs

The first challenge is translating a vague human question like "Show me all pending orders from Colombo delivered in December" into a structured format. 

Instead of asking the LLM to generate raw MongoDB code (which is error-prone and hard to validate), we use OpenAI’s Structured Outputs. By providing a Pydantic-defined schema, we ensure the model returns a validated "Intent JSON" object. This object contains the root collection, fields to select, filters, and sorting requirements, but stays completely database-agnostic.

### Stage 2: The Graph Advantage - Join Discovery with Neo4j

MongoDB is flexible, but its schema relationships are often implicit. To handle complex queries like "customer of an order" or "product within an order’s items," we need to know how collections relate.

We implemented a Metadata Layer using Neo4j. By representing our collections as nodes and relationships (REFERS_TO, EMBEDS) as edges, we can dynamically discover join paths. Our system is designed to handle both **single-hop traversals** (direct relationships like `order -> customer`) and **multi-hop traversals** (complex chains like `orders -> deliveries -> driver`) in the graph. When the Intent JSON mentions a field, the compiler queries Neo4j to find the "Join Recipe"—the exact local and foreign fields needed to execute the necessary `$lookup` sequence in MongoDB.

### Stage 3: Compiling the Pipeline

Once we have the Intent and the Join Recipes, the MongoDB Query Compiler takes over. This is where the heavy lifting happens:

- Path Rewriting: To handle nested data, the compiler automatically rewrites logical paths. For instance, `items.product.name` is transformed into flattened aliases after an `$unwind` stage, making it queryable at the top level.
- Two-Stage Filtering: To optimize performance, the compiler splits `$match` stages. Filters on the root collection are applied early (pre-lookup), while filters on joined data are applied later (post-lookup).
- Aggregation Support: The system supports complex operations like `COUNT` and `GROUP BY`, allowing users to ask "How many orders..." or "What is the total revenue per outlet..."

---

### Key Lessons & Experience

Building this system taught us several valuable lessons:

1. Decouple Schema from Logic: By storing our schema in Neo4j rather than hardcoding it into the LLM prompt, we made the system significantly more maintainable and accurate.
2. Structured Outputs are a Game Changer: Removing the "hallucination" factor from LLMs by enforcing a JSON schema stabilized the entire pipeline.
3. Path Transparency: Managing dot-notation paths in MongoDB aggregation is tricky. Automating the `$unwind` and `$lookup` sequence based on graph paths saved countless hours of manual debugging.

### Conclusion

The combination of LLMs for intent parsing and Graph Databases for schema navigation creates a powerful synergy. We have moved from a world where users need to know `$lookup` syntax to one where they simply ask a question.

This approach proves that with the right architectural "guardrails," AI can be a reliable bridge between human curiosity and complex data storage.

---

#Tech #AI #MongoDB #Neo4j #NLP #DataEngineering
